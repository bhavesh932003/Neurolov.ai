import { NextRequest, NextResponse } from 'next/server';

// Helper function to ensure we always return JSON
const errorResponse = (message: string, status: number = 500, details: any = null) => {
  return new NextResponse(
    JSON.stringify({
      error: message,
      details: details,
    }),
    {
      status,
      headers: {
        'Content-Type': 'application/json',
      },
    }
  );
};

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();

    // Get API keys from environment variables
    const modelslabApiKey = process.env.NEXT_PUBLIC_MODELSLAB_API_KEY;
    const stabilityApiKey = process.env.STABILITY_API_KEY;
    const huggingfaceApiKey = process.env.HUGGINGFACE_API_KEY;

    if (!modelslabApiKey) {
      return NextResponse.json({ error: 'ModelsLab API key not configured' }, { status: 500 });
    }

    // New fields:
    // - attached_image: a base64 string or URL for the attached image (if any)
    // - image_action: action for the attached image (e.g. "enhance", "expand", "edit_prompt")
    const attachedImage = body.attached_image || null;
    const imageAction = body.image_action || null;

    // IMPORTANT: Blob URLs (like those generated by URL.createObjectURL) are client-side only.
    // If the client sends a blob URL, the server cannot access the underlying file.
    // For server-side processing, you must send the actual file data (e.g. as a base64 string).

    // If an attached image and image action are provided, process it locally.
    if (attachedImage && imageAction) {
      if (imageAction === "enhance") {
        // Call the ModelsLab Super Resolution endpoint for enhancing.
        const enhancePayload = {
          key: modelslabApiKey,
          input_image: attachedImage,
          // You can add additional parameters here if required by the endpoint.
        };

        const enhanceResponse = await fetch("https://modelslab.com/api/v6/image_editing/super_resolution", {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
          },
          body: JSON.stringify(enhancePayload),
        });

        const enhanceData = await enhanceResponse.json();
        if (!enhanceResponse.ok) {
          throw new Error(enhanceData.error || "Enhancement failed");
        }

        // Assume the response returns an enhanced image URL or a base64 string in a property like processed_image.
        const enhancedImage = enhanceData.processed_image || attachedImage;
        return NextResponse.json({
          images: [enhancedImage],
          message: `Image processed with ${imageAction}`,
          provider: 'modelslab-super-resolution',
        });
      }

      // For "edit_prompt" simulation: if prompt is provided, simulate editing (e.g. remove a part of the image)
      if (imageAction === "edit_prompt" && body.prompt) {
        // Here, we simulate editing by returning a dummy URL (for example, using dummyimage.com)
        const editedImage = `https://dummyimage.com/1024x1024/cccccc/000.png&text=${encodeURIComponent(body.prompt)}`;
        return NextResponse.json({
          images: [editedImage],
          message: `Image processed with ${imageAction}`,
          provider: 'local-processing',
        });
      }

      // For other actions such as "expand", you can either simulate or return the attached image as-is.
      return NextResponse.json({
        images: [attachedImage],
        message: `Image processed with ${imageAction}`,
        provider: 'local-processing',
      });
    }

    // Otherwise, proceed with ModelsLab generation for text-to-image.
    const modelLabsBody = {
      key: modelslabApiKey,
      prompt: body.prompt,
      negative_prompt: body.negative_prompt || "",
      width: body.width || 512,
      height: body.height || 512,
      samples: body.num_samples || 1,
      safety_checker: body.safety_checker !== false,
      instant_response: false,
      base64: false,
      enhance_prompt: body.enhance_prompt || false,
      enhance_style: body.enhance_style || null,
      // New parameters for image-based operations (if any)
      input_image: attachedImage,
      image_action: imageAction,
    };

    // Try ModelsLab first
    try {
      console.log('Attempting ModelsLab generation:', { ...modelLabsBody, key: '***' });
      const modelLabsResponse = await fetch('https://modelslab.com/api/v6/realtime/text2img', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(modelLabsBody),
      });

      const data = await modelLabsResponse.json();
      console.log('ModelsLab Response:', data);

      // If we have processing status with ETA, wait accordingly
      if (data.status === "processing" && data.eta) {
        const waitTime = data.eta * 1000;
        console.log(`Image processing, ETA: ${data.eta} seconds. Waiting before returning image URLs...`);
        await new Promise((resolve) => setTimeout(resolve, waitTime));

        if (data.future_links && data.future_links.length > 0) {
          console.log('Using future_links after waiting for ETA:', data.future_links);
          return NextResponse.json({
            images: data.future_links,
            message: 'Image generated successfully',
            provider: 'modelslab',
            generationTime: data.generationTime || 0,
            metadata: data.meta || {},
          });
        }

        if (data.fetch_result) {
          console.log(`Fetching result from: ${data.fetch_result}`);
          try {
            const resultResponse = await fetch(data.fetch_result, {
              method: 'GET',
              headers: {
                'Content-Type': 'application/json',
              },
            });
            const resultData = await resultResponse.json();
            console.log('Result after waiting for ETA:', resultData);
            if (resultData.status === "success" && resultData.output && resultData.output.length > 0) {
              return NextResponse.json({
                images: resultData.output,
                message: 'Image generated successfully',
                provider: 'modelslab',
                generationTime: resultData.generationTime || 0,
                metadata: resultData.meta || data.meta || {},
              });
            }
          } catch (error) {
            console.error('Error fetching result:', error);
            if (data.future_links && data.future_links.length > 0) {
              console.log('Fetch failed, falling back to future_links:', data.future_links);
              return NextResponse.json({
                images: data.future_links,
                message: 'Image generated successfully (fallback)',
                provider: 'modelslab',
                generationTime: data.generationTime || 0,
                metadata: data.meta || {},
              });
            }
          }
        }
      }

      if (modelLabsResponse.ok && data.output && data.output.length > 0) {
        return NextResponse.json({
          images: data.output,
          message: 'Image generated successfully',
          provider: 'modelslab',
        });
      }

      throw new Error(data.error || data.message || 'ModelsLab generation failed');
    } catch (modelLabsError) {
      console.warn('ModelsLab failed, trying Stability AI:', modelLabsError);

      // Try Stability AI as first fallback
      if (stabilityApiKey) {
        try {
          const stabilityResponse = await fetch(
            'https://api.stability.ai/v1/generation/stable-diffusion-xl-1024-v1-0/text-to-image',
            {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
                Accept: 'application/json',
                Authorization: `Bearer ${stabilityApiKey}`,
              },
              body: JSON.stringify({
                text_prompts: [
                  { text: body.prompt, weight: 1 },
                  ...(body.negative_prompt ? [{ text: body.negative_prompt, weight: -1 }] : []),
                ],
                cfg_scale: 7,
                height: body.height || 1024,
                width: body.width || 1024,
                samples: body.num_samples || 1,
                steps: 50,
                // Optionally include image-based parameters if provided
                input_image: attachedImage,
                image_action: imageAction,
              }),
            }
          );

          if (!stabilityResponse.ok) {
            const error = await stabilityResponse.json();
            throw new Error(error.message || 'Stability AI generation failed');
          }

          const result = await stabilityResponse.json();
          const images = result.artifacts.map((artifact: any) =>
            `data:image/png;base64,${artifact.base64}`
          );

          return NextResponse.json({
            images,
            message: 'Image generated successfully using Stability AI fallback',
            provider: 'stability',
          });
        } catch (stabilityError) {
          console.warn('Stability AI failed, trying Hugging Face:', stabilityError);
        }
      }

      // Try Hugging Face as final fallback
      if (!huggingfaceApiKey) {
        throw new Error('No available fallback services');
      }

      const sleep = (ms: number) => new Promise((resolve) => setTimeout(resolve, ms));

      const retryHuggingFace = async (maxRetries = 5, initialDelay = 2000) => {
        let lastError;
        for (let i = 0; i < maxRetries; i++) {
          try {
            const huggingFaceResponse = await fetch(
              "https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-2-1",
              {
                headers: { 
                  Authorization: `Bearer ${huggingfaceApiKey}`,
                  'Content-Type': 'application/json',
                },
                method: "POST",
                body: JSON.stringify({
                  inputs: body.prompt,
                  parameters: {
                    negative_prompt: body.negative_prompt,
                    width: body.width || 512,
                    height: body.height || 512,
                    num_inference_steps: 50,
                    num_outputs: body.num_samples || 1,
                    // Optionally include image-based parameters if provided
                    input_image: attachedImage,
                    image_action: imageAction,
                  },
                }),
              }
            );

            if (huggingFaceResponse.status === 503) {
              const error = await huggingFaceResponse.json();
              if (error.error?.includes('loading')) {
                console.log(`Model still loading, attempt ${i + 1}/${maxRetries}. Waiting before retry...`);
                await sleep(initialDelay * Math.pow(2, i));
                continue;
              }
            }

            if (!huggingFaceResponse.ok) {
              const error = await huggingFaceResponse.json();
              throw new Error(error.error || 'Hugging Face generation failed');
            }

            const imageBlob = await huggingFaceResponse.blob();
            const base64Image = Buffer.from(await imageBlob.arrayBuffer()).toString('base64');
            return [`data:image/jpeg;base64,${base64Image}`];
          } catch (error) {
            lastError = error;
            if (!error.message?.includes('loading')) {
              throw error;
            }
            await sleep(initialDelay * Math.pow(2, i));
          }
        }
        throw lastError || new Error('Max retries reached for Hugging Face API');
      };

      const images = await retryHuggingFace();

      return NextResponse.json({
        images,
        message: 'Image generated successfully using Hugging Face fallback',
        provider: 'huggingface',
      });
    }
  } catch (error) {
    console.error('Error in image generation:', error);
    return errorResponse(error.message || 'Failed to generate image', 500);
  }
}
